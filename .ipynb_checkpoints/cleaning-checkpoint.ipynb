{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from nlp_pipeline import *\n",
    "import spacy\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "Before analysis, the tweet text must be processed to limit the number of features to be input into the model as well as transform the text to an optimal format for the model to derive semantic value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I will clean each of the dataframes to include only those related to the products and/or event in question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# read in dataframe containing tweets of user accounts\n",
    "user_dfs = []\n",
    "\n",
    "path_to_json = '/Users/baka_brooks/Documents/metis-projects/project-04/data/user_tweets/'\n",
    "json_pattern = os.path.join(path_to_json,'*.json')\n",
    "file_list = glob.glob(json_pattern)\n",
    "\n",
    "for file in file_list:\n",
    "    data = pd.read_json(file)\n",
    "    user_dfs.append(data)\n",
    "    \n",
    "user_tweets = pd.concat(user_dfs)\n",
    "user_tweets.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data pulled from the media networks and influencers in the tech industry include the last 4000 tweets from each account. To clean this data I will remove retweets and only include tweets within the time range of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69958, 8)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter retweets\n",
    "no_retweets = user_tweets[~user_tweets['text'].str.contains(\"RT\")]\n",
    "no_retweets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tweets pulled based on the related hashtags already only include the tweets surrounding the topics/interest areas I am looking for. I will read in the data and combine with the user tweets, and clean further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10784, 8)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter user tweets to only include those directly before and after the event\n",
    "users_filtered = no_retweets[no_retweets['created_at'] >= '2020-02-01']\n",
    "users_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# read in dataframe containing tweets around event hashtags\n",
    "hashtag_dfs = []\n",
    "\n",
    "path_to_json = '/Users/baka_brooks/Documents/metis-projects/project-04/data/hashtag_tweets/'\n",
    "json_pattern = os.path.join(path_to_json,'*.json')\n",
    "file_list = glob.glob(json_pattern)\n",
    "\n",
    "for file in file_list:\n",
    "    data = pd.read_json(file)\n",
    "    hashtag_dfs.append(data)\n",
    "    \n",
    "hashtag_tweets = pd.concat(hashtag_dfs)\n",
    "hashtag_tweets.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14261, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_str</th>\n",
       "      <th>created_at</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>in_reply_to_status_id_str</th>\n",
       "      <th>in_reply_to_screen_name</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1230537551418658816</td>\n",
       "      <td>2020-02-20 16:59:49+00:00</td>\n",
       "      <td>KenistonHeather</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>#samsungmembers #loveforgalaxy #unpacked #with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1230465254037037056</td>\n",
       "      <td>2020-02-20 12:12:32+00:00</td>\n",
       "      <td>lunchtimetecht1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Ep. 52: @Apple and @SamsungMobileUS Events and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1230194279554977792</td>\n",
       "      <td>2020-02-19 18:15:46+00:00</td>\n",
       "      <td>OmniGrandiose</td>\n",
       "      <td>1.230062e+18</td>\n",
       "      <td>TPfupa</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@TPfupa @SamsungMobileSA @AudreyMoeng @Samsung...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1230144195362525184</td>\n",
       "      <td>2020-02-19 14:56:45+00:00</td>\n",
       "      <td>TheCapeTownGuy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>34</td>\n",
       "      <td>6</td>\n",
       "      <td>Samsung announced the Galaxy Z Flip, their new...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1230053630788530176</td>\n",
       "      <td>2020-02-19 08:56:53+00:00</td>\n",
       "      <td>Samsung_CafeBTM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Get ready for the #GalaxyS20? Ortis Deley gets...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id_str                created_at      screen_name  \\\n",
       "0  1230537551418658816 2020-02-20 16:59:49+00:00  KenistonHeather   \n",
       "1  1230465254037037056 2020-02-20 12:12:32+00:00  lunchtimetecht1   \n",
       "2  1230194279554977792 2020-02-19 18:15:46+00:00    OmniGrandiose   \n",
       "3  1230144195362525184 2020-02-19 14:56:45+00:00   TheCapeTownGuy   \n",
       "4  1230053630788530176 2020-02-19 08:56:53+00:00  Samsung_CafeBTM   \n",
       "\n",
       "   in_reply_to_status_id_str in_reply_to_screen_name  favorite_count  \\\n",
       "0                        NaN                    None              12   \n",
       "1                        NaN                    None               1   \n",
       "2               1.230062e+18                  TPfupa               2   \n",
       "3                        NaN                    None              34   \n",
       "4                        NaN                    None               0   \n",
       "\n",
       "   retweet_count                                               text  \n",
       "0              1  #samsungmembers #loveforgalaxy #unpacked #with...  \n",
       "1              1  Ep. 52: @Apple and @SamsungMobileUS Events and...  \n",
       "2              0  @TPfupa @SamsungMobileSA @AudreyMoeng @Samsung...  \n",
       "3              6  Samsung announced the Galaxy Z Flip, their new...  \n",
       "4              0  Get ready for the #GalaxyS20? Ortis Deley gets...  "
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(hashtag_tweets.shape)\n",
    "hashtag_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2020-02-11 03:48:15+0000', tz='UTC'),\n",
       " Timestamp('2020-02-21 02:38:02+0000', tz='UTC'))"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtag_tweets.created_at.min(), hashtag_tweets.created_at.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tweets gathered via the hashtag range from February 11th to February 21st. This range is the day the event occurred, until the date the data was gathered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge dataframes and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25045, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_str</th>\n",
       "      <th>created_at</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>in_reply_to_status_id_str</th>\n",
       "      <th>in_reply_to_screen_name</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1230661597519138816</td>\n",
       "      <td>2020-02-21 01:12:44+00:00</td>\n",
       "      <td>MKBHD</td>\n",
       "      <td>1.230659e+18</td>\n",
       "      <td>AlexRCamacho1</td>\n",
       "      <td>851</td>\n",
       "      <td>7</td>\n",
       "      <td>@AlexRCamacho1 By not shipping it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1230648790526386176</td>\n",
       "      <td>2020-02-21 00:21:50+00:00</td>\n",
       "      <td>MKBHD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>8204</td>\n",
       "      <td>297</td>\n",
       "      <td>I ordered the Escobar Fold 1.\\nNever got it.\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1230581789619519488</td>\n",
       "      <td>2020-02-20 19:55:36+00:00</td>\n",
       "      <td>MKBHD</td>\n",
       "      <td>1.230581e+18</td>\n",
       "      <td>harshb_</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>@harshb_ @SuperSaf @beebomco @howtomen @verge ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1230566950989942784</td>\n",
       "      <td>2020-02-20 18:56:38+00:00</td>\n",
       "      <td>MKBHD</td>\n",
       "      <td>1.230534e+18</td>\n",
       "      <td>AlijahSimon</td>\n",
       "      <td>170</td>\n",
       "      <td>2</td>\n",
       "      <td>@AlijahSimon @jon_prosser Android n customizat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1230535104214294528</td>\n",
       "      <td>2020-02-20 16:50:05+00:00</td>\n",
       "      <td>MKBHD</td>\n",
       "      <td>1.230534e+18</td>\n",
       "      <td>andrewmartonik</td>\n",
       "      <td>245</td>\n",
       "      <td>2</td>\n",
       "      <td>@andrewmartonik It’s possible my scale isn’t p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id_str                created_at screen_name  \\\n",
       "0  1230661597519138816 2020-02-21 01:12:44+00:00       MKBHD   \n",
       "1  1230648790526386176 2020-02-21 00:21:50+00:00       MKBHD   \n",
       "2  1230581789619519488 2020-02-20 19:55:36+00:00       MKBHD   \n",
       "3  1230566950989942784 2020-02-20 18:56:38+00:00       MKBHD   \n",
       "4  1230535104214294528 2020-02-20 16:50:05+00:00       MKBHD   \n",
       "\n",
       "   in_reply_to_status_id_str in_reply_to_screen_name  favorite_count  \\\n",
       "0               1.230659e+18           AlexRCamacho1             851   \n",
       "1                        NaN                    None            8204   \n",
       "2               1.230581e+18                 harshb_              25   \n",
       "3               1.230534e+18             AlijahSimon             170   \n",
       "4               1.230534e+18          andrewmartonik             245   \n",
       "\n",
       "   retweet_count                                               text  \n",
       "0              7                  @AlexRCamacho1 By not shipping it  \n",
       "1            297  I ordered the Escobar Fold 1.\\nNever got it.\\n...  \n",
       "2              1  @harshb_ @SuperSaf @beebomco @howtomen @verge ...  \n",
       "3              2  @AlijahSimon @jon_prosser Android n customizat...  \n",
       "4              2  @andrewmartonik It’s possible my scale isn’t p...  "
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge dataframes\n",
    "df = pd.concat([users_filtered, hashtag_tweets], ignore_index=True)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I noticed in the `text` column there are newline characters, '\\n'. These need to be removed before moving on to pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove newline characters\n",
    "df = df.replace('\\n','', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23375, 8)"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export cleaned data to a pickle file\n",
    "df.to_pickle(\"original_data.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing\n",
    "Now that the data is cleaned, I can build a corpus from the tweet text and use the NLP pipeline I created to clean, tokenize, stem, and vectorize the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the corpus from the tweet dataframe\n",
    "tweets = df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                    @AlexRCamacho1 By not shipping it\n",
       "1    I ordered the Escobar Fold 1.Never got it.I or...\n",
       "2    @harshb_ @SuperSaf @beebomco @howtomen @verge ...\n",
       "3    @AlijahSimon @jon_prosser Android n customizat...\n",
       "4    @andrewmartonik It’s possible my scale isn’t p...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'><h2>TOKENIZATION WITH SPACY FIRST</h2></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "\n",
    "# tokenize the documents\n",
    "tokens = []\n",
    "for tweet in tweets[:5]:\n",
    "    doc = nlp(tweet)\n",
    "    token = [token.text for token in doc]\n",
    "    tokens.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['@AlexRCamacho1', 'By', 'not', 'shipping', 'it'],\n",
       " ['I',\n",
       "  'ordered',\n",
       "  'the',\n",
       "  'Escobar',\n",
       "  'Fold',\n",
       "  '1.Never',\n",
       "  'got',\n",
       "  'it',\n",
       "  '.',\n",
       "  'I',\n",
       "  'ordered',\n",
       "  'the',\n",
       "  'Escobar',\n",
       "  'Fold',\n",
       "  '2.Never',\n",
       "  'got',\n",
       "  'it.0/10',\n",
       "  'would',\n",
       "  'not',\n",
       "  'recommend'],\n",
       " ['@harshb',\n",
       "  '_',\n",
       "  '@SuperSaf',\n",
       "  '@beebomco',\n",
       "  '@howtomen',\n",
       "  '@verge',\n",
       "  '@CNET',\n",
       "  '@Mrwhosetheboss',\n",
       "  '@androidcentral',\n",
       "  '@sundarpichai',\n",
       "  '@Android',\n",
       "  '…',\n",
       "  'https://t.co/1AyPLPRQQ6'],\n",
       " ['@AlijahSimon',\n",
       "  '@jon_prosser',\n",
       "  'Android',\n",
       "  'n',\n",
       "  'customization',\n",
       "  'n',\n",
       "  'stuff'],\n",
       " ['@andrewmartonik',\n",
       "  'It',\n",
       "  '’s',\n",
       "  'possible',\n",
       "  'my',\n",
       "  'scale',\n",
       "  'is',\n",
       "  'n’t',\n",
       "  'perfect',\n",
       "  ' ',\n",
       "  '¯\\\\_(ツ)_/¯']]"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'><h2>TweetTokenizer</h2></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer(strip_handles=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for tweet in tweets[:5]:\n",
    "    tokens.append(tknzr.tokenize(tweet.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['By', 'not', 'shipping', 'it'],\n",
       " ['I',\n",
       "  'ordered',\n",
       "  'the',\n",
       "  'Escobar',\n",
       "  'Fold',\n",
       "  '1.Never',\n",
       "  'got',\n",
       "  'it',\n",
       "  '.',\n",
       "  'I',\n",
       "  'ordered',\n",
       "  'the',\n",
       "  'Escobar',\n",
       "  'Fold',\n",
       "  '2.Never',\n",
       "  'got',\n",
       "  'it',\n",
       "  '.',\n",
       "  '0/10',\n",
       "  'would',\n",
       "  'not',\n",
       "  'recommend'],\n",
       " ['…', 'https://t.co/1AyPLPRQQ6'],\n",
       " ['Android', 'n', 'customization', 'n', 'stuff'],\n",
       " ['It',\n",
       "  '’',\n",
       "  's',\n",
       "  'possible',\n",
       "  'my',\n",
       "  'scale',\n",
       "  'isn',\n",
       "  '’',\n",
       "  't',\n",
       "  'perfect',\n",
       "  '¯',\n",
       "  '\\\\',\n",
       "  '_',\n",
       "  '(',\n",
       "  'ツ',\n",
       "  ')',\n",
       "  '_',\n",
       "  '/',\n",
       "  '¯']]"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process characters\n",
    "The first step in processing the tweets is to clean the text. This will be done with the following steps:\n",
    "- Remove URLs and hyperlinks.\n",
    "- Remove @ names.\n",
    "- Remove numbers.\n",
    "- Remove special characters, like '\\n'.\n",
    "- Remove punctuation.\n",
    "- Convert to all lowercase letters.\n",
    "- Remove double spaces.  \n",
    "- Remove elongated words.\n",
    "- Remove mentions of the brand and phone names.\n",
    "\n",
    "For the first round of cleaning, I will deal with some of the more Twitter-specific processing; removing hyperlinks, usernames, and hashtags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_one(docs):\n",
    "    \"\"\"\n",
    "    ---\n",
    "    :param docs: Pandas series of texts to pre-process.\n",
    "    :return: Pandas series of cleaned text.\n",
    "    \"\"\"\n",
    "    # remove URLs and hyperlinks\n",
    "    text_nourl = lambda x: re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', x)\n",
    "    # remove @ names\n",
    "    text_noname = lambda x: re.sub('(@[A-Za-z0-9_]+)', '', x)\n",
    "    # remove hashtags\n",
    "    text_nohash = lambda x: re.sub('(#[A-Za-z0-9_]+)', '', x)\n",
    "    \n",
    "    return docs.map(text_nourl).map(text_noname).map(text_nohash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                   By not shipping it\n",
       "1    I ordered the Escobar Fold 1.Never got it.I or...\n",
       "2                                                   … \n",
       "3                      Android n customization n stuff\n",
       "4      It’s possible my scale isn’t perfect  ¯\\_(ツ)_/¯\n",
       "5                                         Same weight!\n",
       "6     They’re actually both 222 grams, just weighed...\n",
       "7    S20 Ultra in the house. Next to an iPhone 11 P...\n",
       "8                                             Shia.gif\n",
       "9                                                    😭\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_text = process_text_one(tweets)\n",
    "new_text[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second round of text processing I will handle the characters. This includes punctuations, numbers, new line characters, and double spaces. I will also convert all of the text to lowercase to enable simpler processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_two(docs):\n",
    "    \"\"\"\n",
    "    ---\n",
    "    :param docs: Pandas series of texts to pre-process.\n",
    "    :return: Pandas series of cleaned text.\n",
    "    \"\"\"\n",
    "    \n",
    "    # remove numbers\n",
    "    text_nonum = lambda x: re.sub(r'\\d+', '', x)\n",
    "    # remove the new line character\n",
    "    text_nonewline = lambda x: re.sub('\\n', '', x)\n",
    "    # remove punctuation\n",
    "    text_nopunct = lambda x: ''.join([char for char in x if char not in string.punctuation])\n",
    "    # convert all letters to lowercase\n",
    "    text_lower = lambda x: x.lower()\n",
    "    # substitute multiple spaces with single space\n",
    "    text_nospaces = lambda x: re.sub(r'\\s+', ' ', x, flags=re.I)\n",
    "    # remove all single characters\n",
    "    text_single = lambda x: re.sub(r'\\s+[a-zA-Z]\\s+', ' ', x)\n",
    "    \n",
    "    return docs.map(text_nonum).map(text_nonewline).map(text_nopunct).map(text_lower).map(text_nospaces).map(text_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                   by not shipping it\n",
       "1    i ordered the escobar fold never got iti order...\n",
       "2                                                   … \n",
       "3                          android customization stuff\n",
       "4             it’s possible my scale isn’t perfect ¯ツ¯\n",
       "5                                          same weight\n",
       "6        they’re actually both grams just weighed them\n",
       "7    s ultra in the house next to an iphone pro max...\n",
       "8                                              shiagif\n",
       "9                                                    😭\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_cleaned = process_text_two(new_text)\n",
    "text_cleaned[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, did you think the cleaning was over? Now that the characters have been processed I must account for \"unnatural language\", AKA elongated words. The beauty of Twitter is that it is also a hotbed of slang and repeating characters, think \"aaaaaahhh\" and \"wooooooow\". Before each tweet can be analyzed, I will create a function to handle these cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_three(docs):  \n",
    "    \"\"\"\n",
    "    ---\n",
    "    :param docs: Pandas series of texts to pre-process.\n",
    "    :return: Pandas series of cleaned text.\n",
    "    \"\"\"\n",
    "    counter = 0\n",
    "    ascii_lowercase = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', \n",
    "                      'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "    # replace 'aaaaaaaahhhhh' with 'aahh'\n",
    "    for letter in ascii_lowercase:\n",
    "        for row_idx, doc in enumerate(docs):\n",
    "            for word_idx, word in enumerate(doc):\n",
    "                original_word = word\n",
    "                while word != word.replace(letter*3, letter*2):\n",
    "                    word = word.replace(letter*3, letter*2) \n",
    "                    docs[row_idx][word_idx] = word\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                   by not shipping it\n",
       "1    i ordered the escobar fold never got iti order...\n",
       "2                                                   … \n",
       "3                          android customization stuff\n",
       "4             it’s possible my scale isn’t perfect ¯ツ¯\n",
       "5                                          same weight\n",
       "6        they’re actually both grams just weighed them\n",
       "7    s ultra in the house next to an iphone pro max...\n",
       "8                                              shiagif\n",
       "9                                                    😭\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_cleaner = process_text_three(text_cleaned)\n",
    "text_cleaner[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step in preparing the data to model is to remove mentions of the brand and phone names. Because I have already removed numerics, I will have to alter my cleaning strategy a bit to remove the correct characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_four(docs):\n",
    "    \"\"\"\n",
    "    ---\n",
    "    :param docs: Pandas series of texts to pre-process.\n",
    "    :return: Pandas series of cleaned text.\n",
    "    \"\"\"\n",
    "    stop_words = ['samsung', 'galaxy', 's ', ' s', 'plus', 'ultra', 'z', 'flip', 'unpacked']\n",
    "    \n",
    "    new_docs = []\n",
    "    for doc in docs:\n",
    "        new_doc = ''.join([word for word in doc if word not in stop_words])\n",
    "        new_docs.append(new_doc.strip())\n",
    "    \n",
    "    return new_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['by not shipping it',\n",
       " 'i ordered the escobar fold never got iti ordered the escobar fold never got it would not recommend',\n",
       " '…',\n",
       " 'android customiation stuff',\n",
       " 'it’s possible my scale isn’t perfect ¯ツ¯',\n",
       " 'same weight',\n",
       " 'they’re actually both grams just weighed them',\n",
       " 's ultra in the house next to an iphone pro max thats big boi',\n",
       " 'shiagif',\n",
       " '😭']"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_cleanest = process_text_four(text_cleaner)\n",
    "text_cleanest[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that each tweet is cleaned, the text can be stemmed and tokenized to be ready for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization & Lemmatization\n",
    "The next step is to lemmatize each character in each tweet to its root word and to tokenize each tweet by word. In the next step when creating the document-term matrix, stop words will also need to be removed in order to maximize semantic value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordNetLemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tweets = [wordNetLemmatizer.lemmatize(text) for text in text_cleanest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input text into original dataframe and export cleaned dataframe\n",
    "cleaned_df = df.copy()\n",
    "cleaned_df['text'] = lemmatized_tweets\n",
    "\n",
    "cleaned_df.to_pickle(\"corpus.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document-Term Matrix\n",
    "Now that the tweets are fully processed, I will create a document-term matrix. I will create a count vector as well as a TF-IDF vector and compare performance of each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaaaa</th>\n",
       "      <th>aaaaah</th>\n",
       "      <th>aaaaand</th>\n",
       "      <th>aaaawwww</th>\n",
       "      <th>aadmi</th>\n",
       "      <th>aakarsha</th>\n",
       "      <th>aapl</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aayush</th>\n",
       "      <th>...</th>\n",
       "      <th>𝗮𝗻𝘆</th>\n",
       "      <th>𝗮𝘄𝗮𝘆</th>\n",
       "      <th>𝗳𝗮𝗰𝘂𝗹𝘁𝘆</th>\n",
       "      <th>𝗳𝗹𝘂𝘅</th>\n",
       "      <th>𝗶𝘀</th>\n",
       "      <th>𝗻𝗼𝘁</th>\n",
       "      <th>𝘁𝗮𝗸𝗲𝘀</th>\n",
       "      <th>𝘁𝗵𝗮𝘁</th>\n",
       "      <th>𝘂𝘀𝗲𝗱</th>\n",
       "      <th>𝙒𝙍𝙄</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>by not shipping it...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i ordered the escobar fold nev...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>…...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>android customiation stuff...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it’s possible my scale isn’t p...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>same weight...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>they’re actually both grams ju...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s ultra in the house next to a...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shiagif...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>😭...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 16203 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   aa  aaaaa  aaaaah  aaaaand  aaaawwww  \\\n",
       "by not shipping it...               0      0       0        0         0   \n",
       "i ordered the escobar fold nev...   0      0       0        0         0   \n",
       "…...                                0      0       0        0         0   \n",
       "android customiation stuff...       0      0       0        0         0   \n",
       "it’s possible my scale isn’t p...   0      0       0        0         0   \n",
       "same weight...                      0      0       0        0         0   \n",
       "they’re actually both grams ju...   0      0       0        0         0   \n",
       "s ultra in the house next to a...   0      0       0        0         0   \n",
       "shiagif...                          0      0       0        0         0   \n",
       "😭...                                0      0       0        0         0   \n",
       "\n",
       "                                   aadmi  aakarsha  aapl  aaron  aayush  ...  \\\n",
       "by not shipping it...                  0         0     0      0       0  ...   \n",
       "i ordered the escobar fold nev...      0         0     0      0       0  ...   \n",
       "…...                                   0         0     0      0       0  ...   \n",
       "android customiation stuff...          0         0     0      0       0  ...   \n",
       "it’s possible my scale isn’t p...      0         0     0      0       0  ...   \n",
       "same weight...                         0         0     0      0       0  ...   \n",
       "they’re actually both grams ju...      0         0     0      0       0  ...   \n",
       "s ultra in the house next to a...      0         0     0      0       0  ...   \n",
       "shiagif...                             0         0     0      0       0  ...   \n",
       "😭...                                   0         0     0      0       0  ...   \n",
       "\n",
       "                                   𝗮𝗻𝘆  𝗮𝘄𝗮𝘆  𝗳𝗮𝗰𝘂𝗹𝘁𝘆  𝗳𝗹𝘂𝘅  𝗶𝘀  𝗻𝗼𝘁  𝘁𝗮𝗸𝗲𝘀  \\\n",
       "by not shipping it...                0     0        0     0   0    0      0   \n",
       "i ordered the escobar fold nev...    0     0        0     0   0    0      0   \n",
       "…...                                 0     0        0     0   0    0      0   \n",
       "android customiation stuff...        0     0        0     0   0    0      0   \n",
       "it’s possible my scale isn’t p...    0     0        0     0   0    0      0   \n",
       "same weight...                       0     0        0     0   0    0      0   \n",
       "they’re actually both grams ju...    0     0        0     0   0    0      0   \n",
       "s ultra in the house next to a...    0     0        0     0   0    0      0   \n",
       "shiagif...                           0     0        0     0   0    0      0   \n",
       "😭...                                 0     0        0     0   0    0      0   \n",
       "\n",
       "                                   𝘁𝗵𝗮𝘁  𝘂𝘀𝗲𝗱  𝙒𝙍𝙄  \n",
       "by not shipping it...                 0     0    0  \n",
       "i ordered the escobar fold nev...     0     0    0  \n",
       "…...                                  0     0    0  \n",
       "android customiation stuff...         0     0    0  \n",
       "it’s possible my scale isn’t p...     0     0    0  \n",
       "same weight...                        0     0    0  \n",
       "they’re actually both grams ju...     0     0    0  \n",
       "s ultra in the house next to a...     0     0    0  \n",
       "shiagif...                            0     0    0  \n",
       "😭...                                  0     0    0  \n",
       "\n",
       "[10 rows x 16203 columns]"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [tweet[:30]+'...' for tweet in cleaned_df.text]\n",
    "\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "doc_word_count = count_vectorizer.fit_transform(cleaned_df.text)\n",
    "dtm_count = pd.DataFrame(doc_word_count.toarray(), index=labels, columns=count_vectorizer.get_feature_names())\n",
    "dtm_count.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export document-term matrix to pickle file\n",
    "dtm_count.to_pickle(\"dtm_count.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaaaa</th>\n",
       "      <th>aaaaah</th>\n",
       "      <th>aaaaand</th>\n",
       "      <th>aaaawwww</th>\n",
       "      <th>aadmi</th>\n",
       "      <th>aakarsha</th>\n",
       "      <th>aapl</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aayush</th>\n",
       "      <th>...</th>\n",
       "      <th>𝗮𝗻𝘆</th>\n",
       "      <th>𝗮𝘄𝗮𝘆</th>\n",
       "      <th>𝗳𝗮𝗰𝘂𝗹𝘁𝘆</th>\n",
       "      <th>𝗳𝗹𝘂𝘅</th>\n",
       "      <th>𝗶𝘀</th>\n",
       "      <th>𝗻𝗼𝘁</th>\n",
       "      <th>𝘁𝗮𝗸𝗲𝘀</th>\n",
       "      <th>𝘁𝗵𝗮𝘁</th>\n",
       "      <th>𝘂𝘀𝗲𝗱</th>\n",
       "      <th>𝙒𝙍𝙄</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>by not shipping it...</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i ordered the escobar fold nev...</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>…...</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>android customiation stuff...</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it’s possible my scale isn’t p...</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>same weight...</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>they’re actually both grams ju...</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s ultra in the house next to a...</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shiagif...</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>😭...</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 16203 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    aa  aaaaa  aaaaah  aaaaand  aaaawwww  \\\n",
       "by not shipping it...              0.0    0.0     0.0      0.0       0.0   \n",
       "i ordered the escobar fold nev...  0.0    0.0     0.0      0.0       0.0   \n",
       "…...                               0.0    0.0     0.0      0.0       0.0   \n",
       "android customiation stuff...      0.0    0.0     0.0      0.0       0.0   \n",
       "it’s possible my scale isn’t p...  0.0    0.0     0.0      0.0       0.0   \n",
       "same weight...                     0.0    0.0     0.0      0.0       0.0   \n",
       "they’re actually both grams ju...  0.0    0.0     0.0      0.0       0.0   \n",
       "s ultra in the house next to a...  0.0    0.0     0.0      0.0       0.0   \n",
       "shiagif...                         0.0    0.0     0.0      0.0       0.0   \n",
       "😭...                               0.0    0.0     0.0      0.0       0.0   \n",
       "\n",
       "                                   aadmi  aakarsha  aapl  aaron  aayush  ...  \\\n",
       "by not shipping it...                0.0       0.0   0.0    0.0     0.0  ...   \n",
       "i ordered the escobar fold nev...    0.0       0.0   0.0    0.0     0.0  ...   \n",
       "…...                                 0.0       0.0   0.0    0.0     0.0  ...   \n",
       "android customiation stuff...        0.0       0.0   0.0    0.0     0.0  ...   \n",
       "it’s possible my scale isn’t p...    0.0       0.0   0.0    0.0     0.0  ...   \n",
       "same weight...                       0.0       0.0   0.0    0.0     0.0  ...   \n",
       "they’re actually both grams ju...    0.0       0.0   0.0    0.0     0.0  ...   \n",
       "s ultra in the house next to a...    0.0       0.0   0.0    0.0     0.0  ...   \n",
       "shiagif...                           0.0       0.0   0.0    0.0     0.0  ...   \n",
       "😭...                                 0.0       0.0   0.0    0.0     0.0  ...   \n",
       "\n",
       "                                   𝗮𝗻𝘆  𝗮𝘄𝗮𝘆  𝗳𝗮𝗰𝘂𝗹𝘁𝘆  𝗳𝗹𝘂𝘅   𝗶𝘀  𝗻𝗼𝘁  𝘁𝗮𝗸𝗲𝘀  \\\n",
       "by not shipping it...              0.0   0.0      0.0   0.0  0.0  0.0    0.0   \n",
       "i ordered the escobar fold nev...  0.0   0.0      0.0   0.0  0.0  0.0    0.0   \n",
       "…...                               0.0   0.0      0.0   0.0  0.0  0.0    0.0   \n",
       "android customiation stuff...      0.0   0.0      0.0   0.0  0.0  0.0    0.0   \n",
       "it’s possible my scale isn’t p...  0.0   0.0      0.0   0.0  0.0  0.0    0.0   \n",
       "same weight...                     0.0   0.0      0.0   0.0  0.0  0.0    0.0   \n",
       "they’re actually both grams ju...  0.0   0.0      0.0   0.0  0.0  0.0    0.0   \n",
       "s ultra in the house next to a...  0.0   0.0      0.0   0.0  0.0  0.0    0.0   \n",
       "shiagif...                         0.0   0.0      0.0   0.0  0.0  0.0    0.0   \n",
       "😭...                               0.0   0.0      0.0   0.0  0.0  0.0    0.0   \n",
       "\n",
       "                                   𝘁𝗵𝗮𝘁  𝘂𝘀𝗲𝗱  𝙒𝙍𝙄  \n",
       "by not shipping it...               0.0   0.0  0.0  \n",
       "i ordered the escobar fold nev...   0.0   0.0  0.0  \n",
       "…...                                0.0   0.0  0.0  \n",
       "android customiation stuff...       0.0   0.0  0.0  \n",
       "it’s possible my scale isn’t p...   0.0   0.0  0.0  \n",
       "same weight...                      0.0   0.0  0.0  \n",
       "they’re actually both grams ju...   0.0   0.0  0.0  \n",
       "s ultra in the house next to a...   0.0   0.0  0.0  \n",
       "shiagif...                          0.0   0.0  0.0  \n",
       "😭...                                0.0   0.0  0.0  \n",
       "\n",
       "[10 rows x 16203 columns]"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "doc_word_tfidf = tfidf_vectorizer.fit_transform(cleaned_df.text)\n",
    "dtm_tfidf = pd.DataFrame(doc_word_tfidf.toarray(), index=labels, columns=tfidf_vectorizer.get_feature_names())\n",
    "dtm_tfidf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export document-term matrix to pickle file\n",
    "dtm_tfidf.to_pickle(\"dtm_tfidf.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('metis': conda)",
   "language": "python",
   "name": "python37464bitmetisconda6ec158bba95c452caae9ce18793916df"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
