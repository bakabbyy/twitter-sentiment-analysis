{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from nlp_pipeline import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "Before analysis, the tweet text must be processed to limit the number of features to be input into the model as well as transform the text to an optimal format for the model to derive semantic value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I will clean each of the dataframes to include only those related to the products and/or event in question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Things to come back and add (running list)</font>  \n",
    "<li><font color='red'>Remove hashtags and @, including characters and text (like usernames)</font></li>\n",
    "<li><font color='red'>Remove URLs --> remove string contains 'http'</font></li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# read in dataframe containing tweets of user accounts\n",
    "user_dfs = []\n",
    "\n",
    "path_to_json = '/Users/baka_brooks/Documents/metis-projects/project-04/data/user_tweets/'\n",
    "json_pattern = os.path.join(path_to_json,'*.json')\n",
    "file_list = glob.glob(json_pattern)\n",
    "\n",
    "for file in file_list:\n",
    "    data = pd.read_json(file)\n",
    "    user_dfs.append(data)\n",
    "    \n",
    "user_tweets = pd.concat(user_dfs)\n",
    "user_tweets.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data pulled from the media networks and influencers in the tech industry include the last 4000 tweets from each account. To clean this data I will remove retweets and only include tweets within the time range of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69958, 8)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter retweets\n",
    "no_retweets = user_tweets[~user_tweets['text'].str.contains(\"RT\")]\n",
    "no_retweets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tweets pulled based on the related hashtags already only include the tweets surrounding the topics/interest areas I am looking for. I will read in the data and combine with the user tweets, and clean further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10784, 8)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter user tweets to only include those directly before and after the event\n",
    "users_filtered = no_retweets[no_retweets['created_at'] >= '2020-02-01']\n",
    "users_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# read in dataframe containing tweets around event hashtags\n",
    "hashtag_dfs = []\n",
    "\n",
    "path_to_json = '/Users/baka_brooks/Documents/metis-projects/project-04/data/hashtag_tweets/'\n",
    "json_pattern = os.path.join(path_to_json,'*.json')\n",
    "file_list = glob.glob(json_pattern)\n",
    "\n",
    "for file in file_list:\n",
    "    data = pd.read_json(file)\n",
    "    hashtag_dfs.append(data)\n",
    "    \n",
    "hashtag_tweets = pd.concat(hashtag_dfs)\n",
    "hashtag_tweets.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14261, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_str</th>\n",
       "      <th>created_at</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>in_reply_to_status_id_str</th>\n",
       "      <th>in_reply_to_screen_name</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1230537551418658816</td>\n",
       "      <td>2020-02-20 16:59:49+00:00</td>\n",
       "      <td>KenistonHeather</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>#samsungmembers #loveforgalaxy #unpacked #with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1230465254037037056</td>\n",
       "      <td>2020-02-20 12:12:32+00:00</td>\n",
       "      <td>lunchtimetecht1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Ep. 52: @Apple and @SamsungMobileUS Events and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1230194279554977792</td>\n",
       "      <td>2020-02-19 18:15:46+00:00</td>\n",
       "      <td>OmniGrandiose</td>\n",
       "      <td>1.230062e+18</td>\n",
       "      <td>TPfupa</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@TPfupa @SamsungMobileSA @AudreyMoeng @Samsung...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1230144195362525184</td>\n",
       "      <td>2020-02-19 14:56:45+00:00</td>\n",
       "      <td>TheCapeTownGuy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>34</td>\n",
       "      <td>6</td>\n",
       "      <td>Samsung announced the Galaxy Z Flip, their new...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1230053630788530176</td>\n",
       "      <td>2020-02-19 08:56:53+00:00</td>\n",
       "      <td>Samsung_CafeBTM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Get ready for the #GalaxyS20? Ortis Deley gets...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id_str                created_at      screen_name  \\\n",
       "0  1230537551418658816 2020-02-20 16:59:49+00:00  KenistonHeather   \n",
       "1  1230465254037037056 2020-02-20 12:12:32+00:00  lunchtimetecht1   \n",
       "2  1230194279554977792 2020-02-19 18:15:46+00:00    OmniGrandiose   \n",
       "3  1230144195362525184 2020-02-19 14:56:45+00:00   TheCapeTownGuy   \n",
       "4  1230053630788530176 2020-02-19 08:56:53+00:00  Samsung_CafeBTM   \n",
       "\n",
       "   in_reply_to_status_id_str in_reply_to_screen_name  favorite_count  \\\n",
       "0                        NaN                    None              12   \n",
       "1                        NaN                    None               1   \n",
       "2               1.230062e+18                  TPfupa               2   \n",
       "3                        NaN                    None              34   \n",
       "4                        NaN                    None               0   \n",
       "\n",
       "   retweet_count                                               text  \n",
       "0              1  #samsungmembers #loveforgalaxy #unpacked #with...  \n",
       "1              1  Ep. 52: @Apple and @SamsungMobileUS Events and...  \n",
       "2              0  @TPfupa @SamsungMobileSA @AudreyMoeng @Samsung...  \n",
       "3              6  Samsung announced the Galaxy Z Flip, their new...  \n",
       "4              0  Get ready for the #GalaxyS20? Ortis Deley gets...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(hashtag_tweets.shape)\n",
    "hashtag_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2020-02-11 03:48:15+0000', tz='UTC'),\n",
       " Timestamp('2020-02-21 02:38:02+0000', tz='UTC'))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtag_tweets.created_at.min(), hashtag_tweets.created_at.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tweets gathered via the hashtag range from February 11th to February 21st. This range is the day the event occurred, until the date the data was gathered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge dataframes and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25045, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_str</th>\n",
       "      <th>created_at</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>in_reply_to_status_id_str</th>\n",
       "      <th>in_reply_to_screen_name</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1230661597519138816</td>\n",
       "      <td>2020-02-21 01:12:44+00:00</td>\n",
       "      <td>MKBHD</td>\n",
       "      <td>1.230659e+18</td>\n",
       "      <td>AlexRCamacho1</td>\n",
       "      <td>851</td>\n",
       "      <td>7</td>\n",
       "      <td>@AlexRCamacho1 By not shipping it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1230648790526386176</td>\n",
       "      <td>2020-02-21 00:21:50+00:00</td>\n",
       "      <td>MKBHD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>8204</td>\n",
       "      <td>297</td>\n",
       "      <td>I ordered the Escobar Fold 1.\\nNever got it.\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1230581789619519488</td>\n",
       "      <td>2020-02-20 19:55:36+00:00</td>\n",
       "      <td>MKBHD</td>\n",
       "      <td>1.230581e+18</td>\n",
       "      <td>harshb_</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>@harshb_ @SuperSaf @beebomco @howtomen @verge ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1230566950989942784</td>\n",
       "      <td>2020-02-20 18:56:38+00:00</td>\n",
       "      <td>MKBHD</td>\n",
       "      <td>1.230534e+18</td>\n",
       "      <td>AlijahSimon</td>\n",
       "      <td>170</td>\n",
       "      <td>2</td>\n",
       "      <td>@AlijahSimon @jon_prosser Android n customizat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1230535104214294528</td>\n",
       "      <td>2020-02-20 16:50:05+00:00</td>\n",
       "      <td>MKBHD</td>\n",
       "      <td>1.230534e+18</td>\n",
       "      <td>andrewmartonik</td>\n",
       "      <td>245</td>\n",
       "      <td>2</td>\n",
       "      <td>@andrewmartonik It’s possible my scale isn’t p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id_str                created_at screen_name  \\\n",
       "0  1230661597519138816 2020-02-21 01:12:44+00:00       MKBHD   \n",
       "1  1230648790526386176 2020-02-21 00:21:50+00:00       MKBHD   \n",
       "2  1230581789619519488 2020-02-20 19:55:36+00:00       MKBHD   \n",
       "3  1230566950989942784 2020-02-20 18:56:38+00:00       MKBHD   \n",
       "4  1230535104214294528 2020-02-20 16:50:05+00:00       MKBHD   \n",
       "\n",
       "   in_reply_to_status_id_str in_reply_to_screen_name  favorite_count  \\\n",
       "0               1.230659e+18           AlexRCamacho1             851   \n",
       "1                        NaN                    None            8204   \n",
       "2               1.230581e+18                 harshb_              25   \n",
       "3               1.230534e+18             AlijahSimon             170   \n",
       "4               1.230534e+18          andrewmartonik             245   \n",
       "\n",
       "   retweet_count                                               text  \n",
       "0              7                  @AlexRCamacho1 By not shipping it  \n",
       "1            297  I ordered the Escobar Fold 1.\\nNever got it.\\n...  \n",
       "2              1  @harshb_ @SuperSaf @beebomco @howtomen @verge ...  \n",
       "3              2  @AlijahSimon @jon_prosser Android n customizat...  \n",
       "4              2  @andrewmartonik It’s possible my scale isn’t p...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge dataframes\n",
    "df = pd.concat([users_filtered, hashtag_tweets], ignore_index=True)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I noticed in the `text` column there are newline characters, '\\n'. These need to be removed before moving on to pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove newline characters\n",
    "df = df.replace('\\n','', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23375, 8)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export cleaned data to a pickle file\n",
    "df.to_pickle(\"original_data.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing\n",
    "Now that the data is cleaned, I can build a corpus from the tweet text and use the NLP pipeline I created to clean, tokenize, stem, and vectorize the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the corpus from the tweet dataframe\n",
    "text = df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                    @AlexRCamacho1 By not shipping it\n",
       "1    I ordered the Escobar Fold 1.Never got it.I or...\n",
       "2    @harshb_ @SuperSaf @beebomco @howtomen @verge ...\n",
       "3    @AlijahSimon @jon_prosser Android n customizat...\n",
       "4    @andrewmartonik It’s possible my scale isn’t p...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process characters\n",
    "The first step in processing the tweets is to clean the text. This will be done with the following steps:\n",
    "- Remove numbers.\n",
    "- Remove special characters, like '\\n'.\n",
    "- Remove punctuation.\n",
    "- Convert to all lowercase letters.\n",
    "- Remove double spaces.  \n",
    "\n",
    "Once each tweet is cleaned, the text can be stemmed and tokenized to be ready for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(docs):\n",
    "    \"\"\"\n",
    "    ---\n",
    "    :param tweets: Pandas series of texts to pre-process.\n",
    "    :return: Pandas series of cleaned text.\n",
    "    \"\"\"\n",
    "    text_nonum = lambda x: re.sub(r'\\d+', '', x)\n",
    "    text_nonewline = lambda x: re.sub('\\n', '', x)\n",
    "    text_nopunct = lambda x: ''.join([char for char in x if char not in string.punctuation])\n",
    "    text_lower = lambda x: x.lower()\n",
    "    text_nodoublespace = lambda x: re.sub('\\s+', ' ', x).strip()\n",
    "    \n",
    "    return docs.map(text_nonum).map(text_nonewline).map(text_nopunct).map(text_lower).map(text_nodoublespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                          alexrcamacho by not shipping it\n",
       "1        i ordered the escobar fold never got iti order...\n",
       "2        harshb supersaf beebomco howtomen verge cnet m...\n",
       "3        alijahsimon jonprosser android n customization...\n",
       "4        andrewmartonik it’s possible my scale isn’t pe...\n",
       "                               ...                        \n",
       "25038    today galaxy s will be presented that means my...\n",
       "25039    galaxys leaks right now ​ httpstcosvcgaxfqa vi...\n",
       "25040    download links unlocked for everyone samsungev...\n",
       "25042    samsung galaxy s and s plus launching todaycli...\n",
       "25044    in honor of the galaxys launch heres a few pho...\n",
       "Name: text, Length: 23375, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clean = process_text(df['text'])\n",
    "text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input text into original dataframe and export cleaned dataframe\n",
    "cleaned_df = df.copy()\n",
    "cleaned_df['text'] = text_clean\n",
    "\n",
    "cleaned_df.to_pickle(\"corpus.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming & Tokenization\n",
    "The next step is to stem each character in each tweet to its root word and to tokenize each tweet by word. During the tokenization phase, stop words will also need to be removed in order to maximize semantic value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the tokenizer to each tweet individually\n",
    "porter_stemmer = PorterStemmer()\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "\n",
    "cleaned_text = []\n",
    "for tweet in text_clean:\n",
    "    cleaned_words = []\n",
    "    for word in tweet_tokenizer.tokenize(tweet):\n",
    "        stemmed_word = porter_stemmer.stem(word)\n",
    "        cleaned_words.append(stemmed_word)\n",
    "    cleaned_text.append(' '.join(cleaned_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alexrcamacho by not ship it',\n",
       " 'i order the escobar fold never got iti order the escobar fold never got it would not recommend',\n",
       " 'harshb supersaf beebomco howtomen verg cnet mrwhosetheboss androidcentr sundarpichai android … httpstcoayplprqq',\n",
       " 'alijahsimon jonpross android n custom n stuff',\n",
       " 'andrewmartonik it ’ s possibl my scale isn ’ t perfect ¯ ツ ¯',\n",
       " 'arnaudducouret same weight',\n",
       " 'andrewmartonik they ’ re actual both gram just weigh them',\n",
       " 's ultra in the hous next to an iphon pro max that a big boi httpstcomvhxerw',\n",
       " 'dnut teslarati shiagif',\n",
       " 'helpertesla teslarati 😭']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document-Term Matrix\n",
    "Now that the tweets are fully processed, I will create a document-term matrix. I will create a count vector as well as a TF-IDF vector and compare performance of each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaaaa</th>\n",
       "      <th>aaaaah</th>\n",
       "      <th>aaaaand</th>\n",
       "      <th>aaaawwww</th>\n",
       "      <th>aadmi</th>\n",
       "      <th>aakarsha</th>\n",
       "      <th>aamaadmiparti</th>\n",
       "      <th>aapkidilli</th>\n",
       "      <th>aapl</th>\n",
       "      <th>...</th>\n",
       "      <th>𝗮𝘄𝗮𝘆</th>\n",
       "      <th>𝗳𝗮𝗰𝘂𝗹𝘁𝘆</th>\n",
       "      <th>𝗳𝗹𝘂𝘅</th>\n",
       "      <th>𝗶𝘀</th>\n",
       "      <th>𝗻𝗼𝘁</th>\n",
       "      <th>𝘁𝗮𝗸𝗲𝘀</th>\n",
       "      <th>𝘁𝗵𝗮𝘁</th>\n",
       "      <th>𝘂𝘀𝗲𝗱</th>\n",
       "      <th>𝙃enlivenphtoalltheboy</th>\n",
       "      <th>𝙒𝙍𝙄enlivenphthirdlookat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alexrcamacho by not ship it...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i order the escobar fold never...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>harshb supersaf beebomco howto...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alijahsimon jonpross android n...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>andrewmartonik it ’ s possibl ...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arnaudducouret same weight...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>andrewmartonik they ’ re actua...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s ultra in the hous next to an...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dnut teslarati shiagif...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>helpertesla teslarati 😭...</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 38151 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   aa  aaaaa  aaaaah  aaaaand  aaaawwww  \\\n",
       "alexrcamacho by not ship it...      0      0       0        0         0   \n",
       "i order the escobar fold never...   0      0       0        0         0   \n",
       "harshb supersaf beebomco howto...   0      0       0        0         0   \n",
       "alijahsimon jonpross android n...   0      0       0        0         0   \n",
       "andrewmartonik it ’ s possibl ...   0      0       0        0         0   \n",
       "arnaudducouret same weight...       0      0       0        0         0   \n",
       "andrewmartonik they ’ re actua...   0      0       0        0         0   \n",
       "s ultra in the hous next to an...   0      0       0        0         0   \n",
       "dnut teslarati shiagif...           0      0       0        0         0   \n",
       "helpertesla teslarati 😭...          0      0       0        0         0   \n",
       "\n",
       "                                   aadmi  aakarsha  aamaadmiparti  aapkidilli  \\\n",
       "alexrcamacho by not ship it...         0         0              0           0   \n",
       "i order the escobar fold never...      0         0              0           0   \n",
       "harshb supersaf beebomco howto...      0         0              0           0   \n",
       "alijahsimon jonpross android n...      0         0              0           0   \n",
       "andrewmartonik it ’ s possibl ...      0         0              0           0   \n",
       "arnaudducouret same weight...          0         0              0           0   \n",
       "andrewmartonik they ’ re actua...      0         0              0           0   \n",
       "s ultra in the hous next to an...      0         0              0           0   \n",
       "dnut teslarati shiagif...              0         0              0           0   \n",
       "helpertesla teslarati 😭...             0         0              0           0   \n",
       "\n",
       "                                   aapl  ...  𝗮𝘄𝗮𝘆  𝗳𝗮𝗰𝘂𝗹𝘁𝘆  𝗳𝗹𝘂𝘅  𝗶𝘀  𝗻𝗼𝘁  \\\n",
       "alexrcamacho by not ship it...        0  ...     0        0     0   0    0   \n",
       "i order the escobar fold never...     0  ...     0        0     0   0    0   \n",
       "harshb supersaf beebomco howto...     0  ...     0        0     0   0    0   \n",
       "alijahsimon jonpross android n...     0  ...     0        0     0   0    0   \n",
       "andrewmartonik it ’ s possibl ...     0  ...     0        0     0   0    0   \n",
       "arnaudducouret same weight...         0  ...     0        0     0   0    0   \n",
       "andrewmartonik they ’ re actua...     0  ...     0        0     0   0    0   \n",
       "s ultra in the hous next to an...     0  ...     0        0     0   0    0   \n",
       "dnut teslarati shiagif...             0  ...     0        0     0   0    0   \n",
       "helpertesla teslarati 😭...            0  ...     0        0     0   0    0   \n",
       "\n",
       "                                   𝘁𝗮𝗸𝗲𝘀  𝘁𝗵𝗮𝘁  𝘂𝘀𝗲𝗱  𝙃enlivenphtoalltheboy  \\\n",
       "alexrcamacho by not ship it...         0     0     0                      0   \n",
       "i order the escobar fold never...      0     0     0                      0   \n",
       "harshb supersaf beebomco howto...      0     0     0                      0   \n",
       "alijahsimon jonpross android n...      0     0     0                      0   \n",
       "andrewmartonik it ’ s possibl ...      0     0     0                      0   \n",
       "arnaudducouret same weight...          0     0     0                      0   \n",
       "andrewmartonik they ’ re actua...      0     0     0                      0   \n",
       "s ultra in the hous next to an...      0     0     0                      0   \n",
       "dnut teslarati shiagif...              0     0     0                      0   \n",
       "helpertesla teslarati 😭...             0     0     0                      0   \n",
       "\n",
       "                                   𝙒𝙍𝙄enlivenphthirdlookat  \n",
       "alexrcamacho by not ship it...                           0  \n",
       "i order the escobar fold never...                        0  \n",
       "harshb supersaf beebomco howto...                        0  \n",
       "alijahsimon jonpross android n...                        0  \n",
       "andrewmartonik it ’ s possibl ...                        0  \n",
       "arnaudducouret same weight...                            0  \n",
       "andrewmartonik they ’ re actua...                        0  \n",
       "s ultra in the hous next to an...                        0  \n",
       "dnut teslarati shiagif...                                0  \n",
       "helpertesla teslarati 😭...                               0  \n",
       "\n",
       "[10 rows x 38151 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [tweet[:30]+'...' for tweet in cleaned_text]\n",
    "\n",
    "countVectorizer = CountVectorizer(stop_words='english')\n",
    "doc_word_count = countVectorizer.fit_transform(cleaned_text)\n",
    "dtm_count = pd.DataFrame(doc_word_count.toarray(), index=labels, columns=countVectorizer.get_feature_names())\n",
    "dtm_count.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export document-term matrix to pickle file\n",
    "dtm_count.to_pickle(\"dtm_count.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaaaa</th>\n",
       "      <th>aaaaah</th>\n",
       "      <th>aaaaand</th>\n",
       "      <th>aaaawwww</th>\n",
       "      <th>aadmi</th>\n",
       "      <th>aakarsha</th>\n",
       "      <th>aamaadmiparti</th>\n",
       "      <th>aapkidilli</th>\n",
       "      <th>aapl</th>\n",
       "      <th>...</th>\n",
       "      <th>𝗮𝘄𝗮𝘆</th>\n",
       "      <th>𝗳𝗮𝗰𝘂𝗹𝘁𝘆</th>\n",
       "      <th>𝗳𝗹𝘂𝘅</th>\n",
       "      <th>𝗶𝘀</th>\n",
       "      <th>𝗻𝗼𝘁</th>\n",
       "      <th>𝘁𝗮𝗸𝗲𝘀</th>\n",
       "      <th>𝘁𝗵𝗮𝘁</th>\n",
       "      <th>𝘂𝘀𝗲𝗱</th>\n",
       "      <th>𝙃enlivenphtoalltheboy</th>\n",
       "      <th>𝙒𝙍𝙄enlivenphthirdlookat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alexrcamacho by not ship it...</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i order the escobar fold never...</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>harshb supersaf beebomco howto...</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alijahsimon jonpross android n...</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>andrewmartonik it ’ s possibl ...</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arnaudducouret same weight...</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>andrewmartonik they ’ re actua...</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s ultra in the hous next to an...</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dnut teslarati shiagif...</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>helpertesla teslarati 😭...</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 38151 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    aa  aaaaa  aaaaah  aaaaand  aaaawwww  \\\n",
       "alexrcamacho by not ship it...     0.0    0.0     0.0      0.0       0.0   \n",
       "i order the escobar fold never...  0.0    0.0     0.0      0.0       0.0   \n",
       "harshb supersaf beebomco howto...  0.0    0.0     0.0      0.0       0.0   \n",
       "alijahsimon jonpross android n...  0.0    0.0     0.0      0.0       0.0   \n",
       "andrewmartonik it ’ s possibl ...  0.0    0.0     0.0      0.0       0.0   \n",
       "arnaudducouret same weight...      0.0    0.0     0.0      0.0       0.0   \n",
       "andrewmartonik they ’ re actua...  0.0    0.0     0.0      0.0       0.0   \n",
       "s ultra in the hous next to an...  0.0    0.0     0.0      0.0       0.0   \n",
       "dnut teslarati shiagif...          0.0    0.0     0.0      0.0       0.0   \n",
       "helpertesla teslarati 😭...         0.0    0.0     0.0      0.0       0.0   \n",
       "\n",
       "                                   aadmi  aakarsha  aamaadmiparti  aapkidilli  \\\n",
       "alexrcamacho by not ship it...       0.0       0.0            0.0         0.0   \n",
       "i order the escobar fold never...    0.0       0.0            0.0         0.0   \n",
       "harshb supersaf beebomco howto...    0.0       0.0            0.0         0.0   \n",
       "alijahsimon jonpross android n...    0.0       0.0            0.0         0.0   \n",
       "andrewmartonik it ’ s possibl ...    0.0       0.0            0.0         0.0   \n",
       "arnaudducouret same weight...        0.0       0.0            0.0         0.0   \n",
       "andrewmartonik they ’ re actua...    0.0       0.0            0.0         0.0   \n",
       "s ultra in the hous next to an...    0.0       0.0            0.0         0.0   \n",
       "dnut teslarati shiagif...            0.0       0.0            0.0         0.0   \n",
       "helpertesla teslarati 😭...           0.0       0.0            0.0         0.0   \n",
       "\n",
       "                                   aapl  ...  𝗮𝘄𝗮𝘆  𝗳𝗮𝗰𝘂𝗹𝘁𝘆  𝗳𝗹𝘂𝘅   𝗶𝘀  𝗻𝗼𝘁  \\\n",
       "alexrcamacho by not ship it...      0.0  ...   0.0      0.0   0.0  0.0  0.0   \n",
       "i order the escobar fold never...   0.0  ...   0.0      0.0   0.0  0.0  0.0   \n",
       "harshb supersaf beebomco howto...   0.0  ...   0.0      0.0   0.0  0.0  0.0   \n",
       "alijahsimon jonpross android n...   0.0  ...   0.0      0.0   0.0  0.0  0.0   \n",
       "andrewmartonik it ’ s possibl ...   0.0  ...   0.0      0.0   0.0  0.0  0.0   \n",
       "arnaudducouret same weight...       0.0  ...   0.0      0.0   0.0  0.0  0.0   \n",
       "andrewmartonik they ’ re actua...   0.0  ...   0.0      0.0   0.0  0.0  0.0   \n",
       "s ultra in the hous next to an...   0.0  ...   0.0      0.0   0.0  0.0  0.0   \n",
       "dnut teslarati shiagif...           0.0  ...   0.0      0.0   0.0  0.0  0.0   \n",
       "helpertesla teslarati 😭...          0.0  ...   0.0      0.0   0.0  0.0  0.0   \n",
       "\n",
       "                                   𝘁𝗮𝗸𝗲𝘀  𝘁𝗵𝗮𝘁  𝘂𝘀𝗲𝗱  𝙃enlivenphtoalltheboy  \\\n",
       "alexrcamacho by not ship it...       0.0   0.0   0.0                    0.0   \n",
       "i order the escobar fold never...    0.0   0.0   0.0                    0.0   \n",
       "harshb supersaf beebomco howto...    0.0   0.0   0.0                    0.0   \n",
       "alijahsimon jonpross android n...    0.0   0.0   0.0                    0.0   \n",
       "andrewmartonik it ’ s possibl ...    0.0   0.0   0.0                    0.0   \n",
       "arnaudducouret same weight...        0.0   0.0   0.0                    0.0   \n",
       "andrewmartonik they ’ re actua...    0.0   0.0   0.0                    0.0   \n",
       "s ultra in the hous next to an...    0.0   0.0   0.0                    0.0   \n",
       "dnut teslarati shiagif...            0.0   0.0   0.0                    0.0   \n",
       "helpertesla teslarati 😭...           0.0   0.0   0.0                    0.0   \n",
       "\n",
       "                                   𝙒𝙍𝙄enlivenphthirdlookat  \n",
       "alexrcamacho by not ship it...                         0.0  \n",
       "i order the escobar fold never...                      0.0  \n",
       "harshb supersaf beebomco howto...                      0.0  \n",
       "alijahsimon jonpross android n...                      0.0  \n",
       "andrewmartonik it ’ s possibl ...                      0.0  \n",
       "arnaudducouret same weight...                          0.0  \n",
       "andrewmartonik they ’ re actua...                      0.0  \n",
       "s ultra in the hous next to an...                      0.0  \n",
       "dnut teslarati shiagif...                              0.0  \n",
       "helpertesla teslarati 😭...                             0.0  \n",
       "\n",
       "[10 rows x 38151 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfVectorizer = TfidfVectorizer(stop_words='english')\n",
    "doc_word_tfidf = tfidfVectorizer.fit_transform(cleaned_text)\n",
    "dtm_tfidf = pd.DataFrame(doc_word_tfidf.toarray(), index=labels, columns=tfidfVectorizer.get_feature_names())\n",
    "dtm_tfidf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export document-term matrix to pickle file\n",
    "dtm_tfidf.to_pickle(\"dtm_tfidf.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('metis': conda)",
   "language": "python",
   "name": "python37464bitmetisconda6ec158bba95c452caae9ce18793916df"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
